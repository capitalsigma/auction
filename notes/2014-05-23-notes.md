##Quick update

I did some time profiling after implementing a few changes to the code. I am now able to parse one day's NYSE data in 2 minutes plus 40 seconds per symbol. For reference, the previous run took roughly 120 minutes plus 1 minute per symbol. 

I also attempted to parse 1 year's worth of data in parallel and was able to parse SPY for all of 2010 in roughly 13.7 hours. I don't know exactly how long this would have take

## Code changes to date

1. I use sed in a subprocess to quickly identify irrelevant lines in the raw data to avoid necessary data preprocessing. 
2. I use gzip in a subprocess to unzip instead of python's native gzip module. 
3. I eliminated code that converted the timestamp from UTC to chicago time, which was a very costly step. 

##Efficiency measurements

To compute the time per symbol, I took the most active 100 stocks (NYSE) of as of 3/10/2010 in terms of share volume from [WSJ][1] and random selected a subset. Here are the stats: 

	numsymbol	time(s)
	1	205.3362529
	2	192.534852
	5	325.184303
	10	569.7983239
	20	852.814091
	50	2212.846639

To compute time to parse multiple days, I use arca_bulk_parser.py to multithread. 

	days	time(s)
	1	565.0809979
	8	1478.741109
	22	8606.806685
	252	49264.65

The above clocking was done with 7 threads. This is because even though there are 8 threads available, ryang2 was running a memory-intensive python script at the same time. Also, memory was fully used up during these runs and there was significant CPU idle time. There are likely ways to multithread more efficiently by using memory more efficiently. 

##Next steps

I suspect we have achieved the most major efficiency gains absent a major code rewrite. We can probably get some further gains by understanding why the CPUs are frequently idle even during the multithreaded runs and optimize how we spawn threads. 

Since book-building requires sequential processing, the fastest possible we can process a file is probably roughly 4-5 minutes. We are roughly double that time right now. A major code rewrite will probably be necessary to get us down to 4-5 minutes. It will probably be more cost-efficient to simply buy more CPUs at this point. 

[1]: http://online.wsj.com/mdc/public/page/2_3021-activnyse-actives-20100310.html?mod=mdc_pastcalendar
