##Quick update

I did some time profiling after implementing a few changes to the code. I am now able to parse one day's NYSE data in 2 minutes plus 40 seconds per symbol. For reference, the previous run took roughly 120 minutes plus 1 minute per symbol. I think we are much close to the production-possibility frontier now for processing one file. 

I also attempted to parse 1 year's worth of data in parallel and was able to parse SPY for all of 2010 in roughly 13.7 hours. This was done with significant CPU idle time while a separate memory-intensive process was running. It may well be possible to speed this up even more. 

## Code changes to date

1. I use sed in a subprocess to quickly identify irrelevant lines in the raw data to avoid necessary data preprocessing. 
2. I use gzip in a subprocess to unzip instead of python's native gzip module. 
3. I eliminated code that converted the timestamp from UTC to chicago time, which was a very costly step. 

##Efficiency measurements

To compute the time per symbol, I took the most active 100 stocks (NYSE) of as of 3/10/2010 in terms of share volume from [WSJ][1] and random selected a subset. Here are the stats: 

	numsymbol	time(s)
	1	205.3362529
	2	192.534852
	5	325.184303
	10	569.7983239
	20	852.814091
	50	2212.846639

To compute time to parse multiple days, I use arca_bulk_parser.py to multithread. 

	days	time(s)
	1	565.0809979
	8	1478.741109
	22	8606.806685
	252	49264.65

The above clocking was done with 7 threads. This is because even though there are 8 threads available, ryang2 was running a memory-intensive python script at the same time. Also, memory was fully used up during these runs and there was significant CPU idle time. There are likely ways to multithread more efficiently by hogging less memory. 

##Assessment

1. It takes roughly 9 minutes to parse 1 day's data for the SPY symbol. The amount of time required for just the gzip/sed step is roughly 4-5 minutes. Since book-building requires sequential processing, this likely represents the fastest possible time. We are roughly double that time right now. I don't see any more quick efficiency gains possible at this point. A major code rewrite is probably necessary to get us more speed for parsing one data file and it will probably at most cut down processing time by one half. 
2. I suspect more gains will come from more efficient parallel processing of multiple files, however. As mentioned earlier CPUs appears to be frequently idle during the runs and there is more optimization to be done along this dimension. We might also explore whether we can use more CPUs (ie. more computers). Potential avenues include processing on the GRID or purchasing more computers. 

##Next Steps

Understand why CPUs are frequently idle even during the multithreaded runs and optimize how we spawn threads. 

[1]: http://online.wsj.com/mdc/public/page/2_3021-activnyse-actives-20100310.html?mod=mdc_pastcalendar
