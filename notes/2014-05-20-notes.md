# An update (2014-05-20)

After a day's work, arca_parser.py now takes 16 minutes to run. The previous run took 2.5 hours, so this is roughly a 90% reduction. I've committed the new code here: https://github.com/mbwong/auction. 

The speed gain comes from quickly removing unused rows before calling the parser. To do so, (1) I use gzip to unzip the .gz file. The gzip output is piped to sed, which then uses regular expressions to identify the useful rows. (2) I then pipe this output to the existing parsing code. In terms of time, (1) takes roughly 4 minutes. (2) takes roughly 12 minutes. All in all this required minimal changes to the existing code. 

## New time profile

         461587790 function calls (461586639 primitive calls) in 989.186 seconds

Notable function calls in the latest run code:

            ncalls  tottime  percall  cumtime  percall filename:lineno(function)
                 1    0.004    0.004  989.187  989.187 arca_parser.py:8(<module>)
                 1  225.061  225.061  989.085  989.085 arca_parser.py:296(parse)
                 1    0.000    0.000    0.000    0.000 arca_parser.py:96(AddRecord)
           2024156   10.736    0.000   30.550    0.000 arca_parser.py:105(__init__)
                 1    0.000    0.000    0.000    0.000 arca_parser.py:120(DeleteRecord)
           2024156    5.954    0.000   11.161    0.000 arca_parser.py:129(__init__)
                 1    0.000    0.000    0.000    0.000 arca_parser.py:142(ModifyRecord)
             16012    0.079    0.000    0.239    0.000 arca_parser.py:149(__init__)
           4064324   35.419    0.000  700.755    0.000 arca_parser.py:183(process_record)
           4064324    7.898    0.000  711.672    0.000 arca_parser.py:416(build_books)
           4064324    8.411    0.000    8.411    0.000 arca_parser.py:55(make_timestamp)
           2040168    9.100    0.000   16.770    0.000 arca_parser.py:69(int_price)
          42731450   16.816    0.000   26.419    0.000 attribute.py:103(fget)
             21490    0.076    0.000   15.027    0.001 table.py:1823(_saveBufferedRows)
              1176    0.006    0.000    5.353    0.005 table.py:2408(flush)
           4064329   10.297    0.000  225.199    0.000 time_utils.py:67(chicago_time)
           4064324    4.752    0.000  320.933    0.000 time_utils.py:74(chicago_time_str)
          24385983  198.616    0.000  201.206    0.000 tz.py:402(_find_ttinfo)
          12192995   11.137    0.000   74.366    0.000 tz.py:427(utcoffset)
          20321642   11.809    0.000  149.785    0.000 tz.py:432(dst)
           4064324  185.952    0.000  311.234    0.000 utils.py:156(make_record)
          81274692   26.023    0.000   37.577    0.000 utils.py:33(get_quantity)
           4080336    6.926    0.000    9.441    0.000 utils.py:39(update_quantity)
           8128648   14.300    0.000   14.582    0.000 utils.py:68(top_n)
           3907121    4.299    0.000    9.638    0.000 utils.py:92(increment_count)
           4064329   24.896    0.000   42.110    0.000 {built-in method fromtimestamp}
           4064330   14.606    0.000  172.792    0.000 {method 'astimezone' of 'datetime.datetime' objects}
           6191950   33.802    0.000   33.802    0.000 {method 'all' of 'numpy.ndarray' objects}
           3907122    2.213    0.000   17.202    0.000 {method 'append' of 'tables.tableExtension.Row' objects}

## My next steps

- Understand the time-stamping, which seems to be a large bottleneck. 
- Understand the book-building algorithm, which is the next largest bottleneck. 

## Assessment

I am optimistic that there are easy-to-achieve speed improvements wrt the time-stamping. Further improvements may be more difficult, however. 
