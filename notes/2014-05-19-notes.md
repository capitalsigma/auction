Here's my progress on the book-building code review.
 
I did some time-profiling of the existing code by running the following cmd.
 
    python -m cProfile /datastore/mbwong/auction/py/arca_parser.py -d 20100310 -s AAPL SPY > cprofile.log
 
## Basic facts
 
The raw zip file is 3.6GB. The file has 270 million rows, a fraction of which is relevant when parsing for a given symbol. For example, AAPL and SPY together only accounted for 4 million rows.
 
## Time profile of arca_parser.py
 
Total run time = 9000 seconds (150 minutes)
 
Preprocessing-data function calls (All rows)

    1100 seconds: gzip.readline()
    2600 seconds: re.split()
    1700 seconds: DeleteRecord()
    650 seconds: AddRecord()
        540 seconds: make_timestamp()
        845 seconds: int_price()
 
Order-book-building function calls (Only relevant rows)

    720 seconds: build_books()
 
## Assessment
 
The majority of computing time is spent sequentially reading and processing lines from the zipped file that are not actually relevant to the symbols of interest. I expect there are substantial time-savings possible here by avoiding preprocessing unnecessary rows.
 
## Next steps
 
Spend time exploring the use of an alternative software program to handle data pre-processing, e.g. Stata, to quickly remove the irrelevant rows. In my experience, Stata is very fast at this kind of task since it is not shy to hoard RAM and it automatically employs parallel processes. Other complementary avenues include switching to using tar to unzip files and python's csv module to process csv.
